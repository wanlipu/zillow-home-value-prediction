{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages and Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from scipy import sparse,stats\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, pipeline, metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = pd.read_csv('data/properties_2016.csv')\n",
    "# pd.read_csv(sio, dtype={\"user_id\": int, \"username\": object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_2016_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(properties.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join train Data Frame with properties Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train.merge(properties, how='left', on='parcelid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target: Logerror - Histogram including all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "plt.hist(train_df.logerror, bins = 50)\n",
    "plt.xlabel('logerror', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Target : Logerror - Histogram (1st-99th percentile data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upperlimit = np.percentile(train_df.logerror, 99)\n",
    "lowerlimit = np.percentile(train_df.logerror, 1)\n",
    "\n",
    "plt.figure(figsize = (16, 6))\n",
    "plt.hist(train_df.query('logerror > {} and logerror < {}'.format(lowerlimit, upperlimit)).logerror, \n",
    "         bins = 50)\n",
    "plt.xlabel('logerror', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key feature: Tax Value - taxvaluedollarcnt - Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when there is NaN in your column, use query to remove NaN for data display \n",
    "plt.figure(figsize = (16, 6))\n",
    "plt.hist(train_df.query('taxvaluedollarcnt == taxvaluedollarcnt').taxvaluedollarcnt, \n",
    "         bins = 50)\n",
    "#plt.hist(train_df.taxvaluedollarcnt, bins = 50)\n",
    "plt.xlabel('taxvaluedollarcnt', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key feature: Tax Value - taxvaluedollarcnt - Histogram (1st-99th percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upperlimit = np.percentile(train_df.query('taxvaluedollarcnt == taxvaluedollarcnt').taxvaluedollarcnt.values, 99)\n",
    "lowerlimit = np.percentile(train_df.query('taxvaluedollarcnt == taxvaluedollarcnt').taxvaluedollarcnt.values, 1)\n",
    "\n",
    "plt.figure(figsize = (16, 6))\n",
    "plt.hist(train_df.query('taxvaluedollarcnt > {} and taxvaluedollarcnt < {}'.format(lowerlimit, upperlimit)).taxvaluedollarcnt, \n",
    "         bins = 50)\n",
    "plt.xlabel('taxvaluedollarcnt', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Feature: Lot size - lotsizesquarefeet - Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "plt.hist(train_df.query('lotsizesquarefeet == lotsizesquarefeet').lotsizesquarefeet, \n",
    "         bins = 50)\n",
    "plt.xlabel('lotsizesquarefeet', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Feature: Lot size - lotsizesquarefeet - Histogram (< 30,000 sqft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "plt.hist(train_df.query('lotsizesquarefeet > 0 and lotsizesquarefeet < 30000').lotsizesquarefeet, \n",
    "         bins = 50)\n",
    "plt.xlabel('lotsizesquarefeet', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Feature: Built Year - yearbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "plt.hist(train_df.query('yearbuilt == yearbuilt').yearbuilt, \n",
    "         bins = 50)\n",
    "plt.xlabel('yearbuilt', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate EDA: logerror vs. tax value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 16))\n",
    "sns.jointplot('taxvaluedollarcnt', \n",
    "              'logerror', \n",
    "              train_df.query('taxvaluedollarcnt==taxvaluedollarcnt'), \n",
    "              size = 10, \n",
    "              kind='reg', )\n",
    "plt.xlabel('taxvaluedollarcnt', fontsize = 16)\n",
    "plt.ylabel('logerror', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no clear relation between logerror and taxvaluedollarcnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add abs_logerror as the new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['abs_logerror'] = train_df.logerror.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 16))\n",
    "sns.jointplot('taxvaluedollarcnt', \n",
    "              'abs_logerror', \n",
    "              train_df.query('taxvaluedollarcnt == taxvaluedollarcnt'), \n",
    "              size = 10, \n",
    "              kind = 'reg')\n",
    "plt.xlabel('taxvaluedollarcnt', fontsize = 16)\n",
    "plt.ylabel('abs_logerror', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for high value house, taxvaluedollarcnt could help a bit to predict logerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matplotlib scatter plot with alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.scatter(train_df.taxvaluedollarcnt, \n",
    "            train_df.abs_logerror, \n",
    "            alpha=0.1)\n",
    "plt.xlabel('taxvaluedollarcnt', fontsize = 16)\n",
    "plt.ylabel('abs_logerror', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zoom into detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.scatter(train_df.taxvaluedollarcnt, \n",
    "            train_df.abs_logerror, \n",
    "            alpha=0.1)\n",
    "plt.xlabel('taxvaluedollarcnt', fontsize = 16)\n",
    "plt.ylabel('abs_logerror', fontsize = 16)\n",
    "plt.xlim(0, 1000000)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate EDA: logerror vs. built year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple cross-plot between abs_logerror and built year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.scatter(train_df.yearbuilt, \n",
    "            train_df.abs_logerror, \n",
    "            alpha=0.1)\n",
    "plt.xlabel('year', fontsize = 16)\n",
    "plt.ylabel('abs_logerror', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similar cross-plot with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (16, 6))\n",
    "sns.jointplot('yearbuilt', \n",
    "              'abs_logerror', \n",
    "              train_df.query('yearbuilt==yearbuilt'), \n",
    "              size = 9, \n",
    "              kind='reg')\n",
    "plt.xlabel('year', fontsize = 16)\n",
    "plt.ylabel('abs_logerror', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate abs_logerror regarding built year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_logerrorAggYear = train_df.groupby('yearbuilt').abs_logerror.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.scatter(train_df_logerrorAggYear.yearbuilt, \n",
    "            train_df_logerrorAggYear.abs_logerror,\n",
    "           alpha = 0.8)\n",
    "plt.xlabel('year', fontsize = 16)\n",
    "plt.ylabel('abs_logerror', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('yearbuilt', \n",
    "              'abs_logerror', \n",
    "              train_df_logerrorAggYear, \n",
    "              size = 10, \n",
    "              kind = 'reg')\n",
    "plt.xlabel('year', fontsize = 16)\n",
    "plt.ylabel('abs_logerror', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 10))\n",
    "\n",
    "plt.scatter(train_df_logerrorAggYear.yearbuilt, \n",
    "            train_df_logerrorAggYear.abs_logerror, \n",
    "            label='data', \n",
    "            color='red', \n",
    "            marker='o', \n",
    "            alpha=.5)\n",
    "# sns.regplot(train_df_logerrorAggYear.yearbuilt, \n",
    "#             train_df_logerrorAggYear.abs_logerror, \n",
    "#             scatter=None, \n",
    "#             color='blue', \n",
    "#             label='order 1')\n",
    "sns.regplot(train_df_logerrorAggYear.yearbuilt, \n",
    "            train_df_logerrorAggYear.abs_logerror, \n",
    "            scatter=None, \n",
    "            order=2, \n",
    "            color='green', \n",
    "            label='order 2')\n",
    "sns.regplot(train_df_logerrorAggYear.yearbuilt, \n",
    "            train_df_logerrorAggYear.abs_logerror, \n",
    "            scatter=None, \n",
    "            order=3, \n",
    "            color='purple', \n",
    "            label='order 3')\n",
    "\n",
    "plt.xlabel('year', fontsize = 16)\n",
    "plt.ylabel('abs_logerror', fontsize = 16)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new houses are easier to predict, it makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Transaction Year and Transaction Month from transactiondate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_copy = train_df.copy()\n",
    "train_df_copy['transactiondate'] = pd.to_datetime(train_df_copy['transactiondate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_copy['transactionyear'] = train_df_copy['transactiondate'].dt.year\n",
    "train_df_copy['transactionmonth'] = train_df_copy['transactiondate'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate abs_logerror regarding Transaction Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_logerrorTransactionyear = train_df_copy.groupby('transactionyear').abs_logerror.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_logerrorTransactionyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate abs_logerror regarding Transaction Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_logerrorTransactionmonth = train_df_copy.groupby('transactionmonth').abs_logerror.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_logerrorTransactionmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "plt.plot(train_df_logerrorTransactionmonth.transactionmonth, \n",
    "         train_df_logerrorTransactionmonth.abs_logerror, \n",
    "         color='red', \n",
    "         marker='o',\n",
    "         markersize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate EDA:  Tax Amount vs. Lot Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('calculatedfinishedsquarefeet',\n",
    "              'taxamount',\n",
    "              train_df,\n",
    "              size=10,\n",
    "              kind='reg')\n",
    "plt.xlabel('Lot Size', fontsize = 16)\n",
    "plt.ylabel('Tax Amount', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate EDA: Tax Amount vs. Tax Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('taxvaluedollarcnt',\n",
    "              'taxamount',\n",
    "              train_df,\n",
    "              size=10,\n",
    "              kind='reg')\n",
    "plt.xlabel('Tax Value', fontsize = 16)\n",
    "plt.ylabel('Tax Amount', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate EDA: logerror vs. architecturalstyletypeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(data=train_df,\n",
    "            x='architecturalstyletypeid',\n",
    "            y='abs_logerror')\n",
    "plt.xlabel('Architecture Style', fontsize = 16)\n",
    "plt.ylabel('Logerror', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.violinplot(data=train_df,\n",
    "            x='architecturalstyletypeid',\n",
    "            y='abs_logerror')\n",
    "plt.xlabel('Architecture Style', fontsize = 16)\n",
    "plt.ylabel('Logerror', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zillow model is not ideal for architectural style 7, more time could be spent on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(data=train_df,\n",
    "            x='architecturalstyletypeid',\n",
    "            y='taxvaluedollarcnt')\n",
    "plt.xlabel('Architecture Style', fontsize = 16)\n",
    "plt.ylabel('Tax Value', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.violinplot(data=train_df,\n",
    "            x='architecturalstyletypeid',\n",
    "            y='taxvaluedollarcnt')\n",
    "plt.xlabel('Architecture Style', fontsize = 16)\n",
    "plt.ylabel('Tax Value', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_df = train_df.isnull().sum(axis = 0).reset_index()\n",
    "missing_value_df.columns = ['column_name', 'missing_count']\n",
    "missing_value_df = missing_value_df.loc[missing_value_df['missing_count'] > 0]\n",
    "missing_value_df = missing_value_df.sort_values(by = 'missing_count')\n",
    "\n",
    "index = np.arange(missing_value_df.shape[0])\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(12,20))\n",
    "rects = ax.barh(index, missing_value_df.missing_count.values / train_df.shape[0], color='blue')\n",
    "ax.set_yticks(index)\n",
    "ax.set_yticklabels(missing_value_df.column_name.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Fraction of missing values\")\n",
    "ax.set_title(\"Fractions of missing values in each column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Variables into different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = list(train_df.dtypes[train_df.dtypes=='object'].index)\n",
    "num_vars = list(train_df.dtypes[train_df.dtypes=='int64'].index) + list(train_df.dtypes[train_df.dtypes=='float64'].index)\n",
    "\n",
    "id_var = 'id'\n",
    "target_var = 'logerror'\n",
    "num_vars.remove('parcelid')\n",
    "num_vars.remove('logerror')\n",
    "num_vars.remove('abs_logerror')\n",
    "cat_vars.remove('transactiondate')\n",
    "\n",
    "dt_vars=['transactiondate']\n",
    "\n",
    "print(\"Categorical features:\", cat_vars)\n",
    "print(\"Numerical features:\", num_vars)\n",
    "print(\"Datetime features:\", dt_vars)\n",
    "print(\"ID: {}, target: {}\" .format( id_var, target_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties['finished_sq_ratio'] = properties[['calculatedfinishedsquarefeet','lotsizesquarefeet']].apply(\n",
    "    lambda x : x[0]/x[1] if x[1] > 0 else -999999, axis = 1)\n",
    "\n",
    "properties['taxvalue_per_sq'] = properties[['taxvaluedollarcnt','calculatedfinishedsquarefeet']].apply(\n",
    "    lambda x : x[0]/x[1] if x[1] > 0 else -999999, axis = 1)\n",
    "\n",
    "properties['structure_tax_ratio'] = properties[['structuretaxvaluedollarcnt','taxvaluedollarcnt']].apply(\n",
    "    lambda x : x[0]/x[1] if x[1] > 0 else -999999, axis = 1)\n",
    "\n",
    "properties['landtax_per_sq'] = properties[['landtaxvaluedollarcnt','lotsizesquarefeet']].apply(\n",
    "    lambda x : x[0]/x[1] if x[1] > 0 else -999999, axis = 1)\n",
    "\n",
    "properties['assessmentyear_to_builtyear']= properties[['assessmentyear','yearbuilt']].apply(\n",
    "    lambda x : x[0]-x[1] if x[0] > 0 and x[1] > 0 else -999999, axis = 1)\n",
    "\n",
    "\n",
    "num_to_num_vars = ['finished_sq_ratio','taxvalue_per_sq','structure_tax_ratio',\n",
    "                   'landtax_per_sq','assessmentyear_to_builtyear']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update train_df with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, properties[num_to_num_vars + ['parcelid']], how='left', on='parcelid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical features: Label Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding only works for tree models, will need One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE = preprocessing.LabelEncoder()\n",
    "\n",
    "LE_vars=[]\n",
    "LE_map=dict()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Label Encoding {}\".format(cat_var))\n",
    "    LE_var = cat_var + '_le'\n",
    "    properties[LE_var] = LE.fit_transform(properties[cat_var].astype(str).fillna('none'))\n",
    "    LE_vars.append(LE_var)\n",
    "    LE_map[cat_var]=LE.classes_\n",
    "    \n",
    "print (\"Label-encoded feaures: {}\".format(LE_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical features: One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "OHE.fit(properties[LE_vars])\n",
    "OHE_sparse=OHE.transform(properties[LE_vars])\n",
    "                                   \n",
    "print ('One-hot-encoding finished in {} seconds'.format(time.time()-start))\n",
    "\n",
    "OHE_vars = [var[:-3] + '_' + str(level).replace(' ','_')\\\n",
    "                for var in cat_vars for level in LE_map[var] ]\n",
    "\n",
    "print (\"OHE_sparse size :\" ,OHE_sparse.shape)\n",
    "print (\"One-hot encoded catgorical feature samples : {}\".format(OHE_vars[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with minimum features\n",
    "train_df = train.merge(properties, how='left', on='parcelid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vars = num_vars + LE_vars \n",
    "train_x = train_df[full_vars]\n",
    "train_y = train_df['logerror'].values.astype(np.float32)\n",
    "\n",
    "test_x = properties[full_vars]\n",
    "\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 4,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': 1,\n",
    "    'seed': 1234\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "dtest = xgb.DMatrix(test_x)\n",
    "\n",
    "# cross-validation\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   nfold=5,\n",
    "                   num_boost_round=10000,\n",
    "                   early_stopping_rounds=50,\n",
    "                   verbose_eval=10, \n",
    "                   show_stdv=False,\n",
    "                   seed = 1234\n",
    "                  )\n",
    "\n",
    "## best score and best round\n",
    "best_iteration = len(cv_result)\n",
    "best_score = cv_result['test-mae-mean'].min()\n",
    "print(\"Best score {}, best iteration {}\".format(best_score,best_iteration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(dict(xgb_params, silent = 1), dtrain, num_boost_round = best_iteration)\n",
    "pred = model.predict(dtest)\n",
    "y_pred=[]\n",
    "\n",
    "for i,predict in enumerate(pred):\n",
    "    y_pred.append(str(round(predict,4)))\n",
    "y_pred=np.array(y_pred)\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "output.to_csv('outputs/sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index = False)\n",
    "\n",
    "print (\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imporantce = pd.Series(model.get_fscore()).sort_values(ascending = True)\n",
    "feature_imporantce.plot.barh(x='feature_name',figsize=(8,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vars = num_vars + LE_vars + num_to_num_vars\n",
    "train_x = train_df[full_vars]\n",
    "train_y = train_df['logerror'].values.astype(np.float32)\n",
    "\n",
    "test_x = properties[full_vars]\n",
    "\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 4,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'silent': 1,\n",
    "    'seed': 1234\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "dtest = xgb.DMatrix(test_x)\n",
    "\n",
    "# cross-validation\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   nfold=5,\n",
    "                   num_boost_round=10000,\n",
    "                   early_stopping_rounds=50,\n",
    "                   verbose_eval=10, \n",
    "                   show_stdv=False,\n",
    "                   seed = 1234\n",
    "                  )\n",
    "\n",
    "## best score and best round\n",
    "best_iteration = len(cv_result)\n",
    "best_score = cv_result['test-mae-mean'].min()\n",
    "print(\"Best score {}, best iteration {}\".format(best_score,best_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(dict(xgb_params, silent = 1), dtrain, num_boost_round = best_iteration)\n",
    "pred = model.predict(dtest)\n",
    "y_pred=[]\n",
    "\n",
    "for i,predict in enumerate(pred):\n",
    "    y_pred.append(str(round(predict,4)))\n",
    "y_pred=np.array(y_pred)\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "output.to_csv('outputs/sub_with_engineered_features{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "print (\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imporantce = pd.Series(model.get_fscore()).sort_values(ascending = True)\n",
    "feature_imporantce.plot.barh(x='feature_name',figsize=(8,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGBoost  tuning\n",
    "## Manuanl Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_vars = num_vars + LE_vars\n",
    "train_df = pd.merge(train, properties,\n",
    "                     how='left', on='parcelid')\n",
    "\n",
    "train_x = train_df[full_vars]\n",
    "train_y = train_df['logerror'].values.astype(np.float32)\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, train_y)\n",
    "xgtest = xgb.DMatrix(properties[full_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_scores = pd.DataFrame()\n",
    "scores = []\n",
    "\n",
    "for max_depth in [3,4,5,6,7,8,9,10]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'reg:linear'\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = 1\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = 1\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgtrain,\n",
    "                        num_boost_round=1000000,\n",
    "                        nfold=5,\n",
    "                        metrics={'mae'},\n",
    "                        seed=1234,\n",
    "                        callbacks=[xgb.callback.early_stop(50)],\n",
    "                        verbose_eval=50)\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mae-mean'].min()\n",
    "    print (max_depth,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_max_depth = int(pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['max_depth'].values[0])\n",
    "print ('best max_depth is', best_max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_scores = pd.DataFrame()\n",
    "scores = []\n",
    "\n",
    "for min_child_weight in [1,3,10,30,50,75,100]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'reg:linear'\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = 1\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgtrain,\n",
    "                        num_boost_round=1000000,\n",
    "                        nfold=5,\n",
    "                        metrics={'mae'},\n",
    "                        seed=1234,\n",
    "                        callbacks=[xgb.callback.early_stop(50)],\n",
    "                        verbose_eval=50)\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mae-mean'].min()\n",
    "    print (min_child_weight,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_min_child_weight = int(pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['min_child_weight'].values[0])\n",
    "print ('best min_child_weight is', best_min_child_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_scores = pd.DataFrame()\n",
    "scores = []\n",
    "\n",
    "for colsample_bytree in [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'reg:linear'\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = best_min_child_weight\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = 1\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgtrain,\n",
    "                        num_boost_round=1000000,\n",
    "                        nfold=5,\n",
    "                        metrics={'mae'},\n",
    "                        seed=1234,\n",
    "                        callbacks=[xgb.callback.early_stop(50)],\n",
    "                        verbose_eval=50)\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mae-mean'].min()\n",
    "    print (colsample_bytree,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_colsample_bytree = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).\\\n",
    "                sort_values(by='score',ascending=True)['colsample_bytree'].values[0]\n",
    "print ('best colsample_bytree is', best_colsample_bytree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_scores = pd.DataFrame()\n",
    "scores = []\n",
    "\n",
    "for subsample in [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'reg:linear'\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = best_min_child_weight\n",
    "    params['colsample_bytree'] = best_colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgtrain,\n",
    "                        num_boost_round=1000000,\n",
    "                        nfold=5,\n",
    "                        metrics={'mae'},\n",
    "                        seed=1234,\n",
    "                        callbacks=[xgb.callback.early_stop(50)],\n",
    "                        verbose_eval=50)\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mae-mean'].min()\n",
    "    print (subsample,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_subsample = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).\\\n",
    "                sort_values(by='score',ascending=True)['subsample'].values[0]\n",
    "print ('best subsample is', best_subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_scores = pd.DataFrame()\n",
    "scores = []\n",
    "\n",
    "for gamma in [0,0.1,0.2,0.5,1,1.25,1.5,1.75,2]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'reg:linear'\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = best_min_child_weight\n",
    "    params['colsample_bytree'] = best_colsample_bytree\n",
    "    params['subsample'] = best_subsample\n",
    "    params['gamma'] = gamma\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgtrain,\n",
    "                        num_boost_round=1000000,\n",
    "                        nfold=5,\n",
    "                        metrics={'mae'},\n",
    "                        seed=1234,\n",
    "                        callbacks=[xgb.callback.early_stop(50)],\n",
    "                        verbose_eval=50)\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mae-mean'].min()\n",
    "    print (gamma,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_gamma = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).\\\n",
    "                    sort_values(by='score',ascending=True)['gamma'].values[0]\n",
    "print ('best gamma is', best_subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model training with manually tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['objective'] = 'reg:linear'\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = best_max_depth\n",
    "params['min_child_weight'] = best_min_child_weight\n",
    "params['colsample_bytree'] = best_colsample_bytree\n",
    "params['subsample'] = best_subsample\n",
    "params['gamma'] = best_gamma\n",
    "params['seed']=1234\n",
    "\n",
    "\n",
    "\n",
    "model = xgb.train(params, xgtrain, num_boost_round=best_iteration)\n",
    "pred = model.predict(xgtest)\n",
    "y_pred=[]\n",
    "\n",
    "for i,predict in enumerate(pred):\n",
    "    y_pred.append(str(round(predict,4)))\n",
    "y_pred=np.array(y_pred)\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "output.to_csv('outputs/sub_manually_tuned{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "print (\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma):\n",
    "    params = dict()\n",
    "    params['objective'] = 'reg:linear'\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = int(max_depth )   \n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = gamma\n",
    "    params['verbose_eval'] = True    \n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain,\n",
    "                       num_boost_round=100000,\n",
    "                       nfold=5,\n",
    "                       metrics={'mae'},\n",
    "                       seed=1234,\n",
    "                       callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "    return -cv_result['test-mae-mean'].min()\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(xgb_evaluate, \n",
    "                             {'max_depth': (2, 5),\n",
    "                              'min_child_weight': (0, 100),\n",
    "                              'colsample_bytree': (0.1, 1),\n",
    "                              'subsample': (0.7, 1),\n",
    "                              'gamma': (0, 2)\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "xgb_BO.maximize(init_points=8, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BO_scores = pd.DataFrame(xgb_BO.res['all']['params'])\n",
    "BO_scores['score'] = pd.DataFrame(xgb_BO.res['all']['values'])\n",
    "BO_scores = BO_scores.sort_values(by='score',ascending=False).reset_index()\n",
    "BO_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params = dict()\n",
    "\n",
    "best_params['max_depth'] = int(BO_scores['max_depth'][0])\n",
    "best_params['min_child_weight'] = int(BO_scores['min_child_weight'][0])\n",
    "best_params['colsample_bytree'] = BO_scores['colsample_bytree'][0]\n",
    "best_params['subsample'] = BO_scores['subsample'][0]\n",
    "best_params['gamma'] = BO_scores['gamma'][0]\n",
    "\n",
    "best_params['objective'] = 'reg:linear'\n",
    "best_params['eta'] = 0.1\n",
    "best_params['seed'] = 1234\n",
    "\n",
    "print (best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the models with auto-tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params = dict()\n",
    "\n",
    "best_params['max_depth'] = int(BO_scores['max_depth'][0])\n",
    "best_params['min_child_weight'] = int(BO_scores['min_child_weight'][0])\n",
    "best_params['colsample_bytree'] = BO_scores['colsample_bytree'][0]\n",
    "best_params['subsample'] = BO_scores['subsample'][0]\n",
    "best_params['gamma'] = BO_scores['gamma'][0]\n",
    "\n",
    "best_params['objective'] = 'reg:linear'\n",
    "best_params['eta'] = 0.1\n",
    "best_params['seed'] = 1234\n",
    "\n",
    "\n",
    "cv_result = xgb.cv(best_params, xgtrain,\n",
    "                   num_boost_round=100000,\n",
    "                   nfold=5,\n",
    "                   metrics={'mae'},\n",
    "                   seed=1234,\n",
    "                   callbacks=[xgb.callback.early_stop(50)], \n",
    "                  verbose_eval=50)\n",
    "\n",
    "best_iteration = len(cv_result)\n",
    "best_score = cv_result['test-mae-mean'].min()\n",
    "\n",
    "print(\"Best score {}, best iteration {}\".format(best_score,best_iteration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgtest = xgb.DMatrix(properties[full_vars].values,missing=-999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.train(best_params, xgtrain, num_boost_round=best_iteration)\n",
    "pred = model.predict(xgtest)\n",
    "y_pred=[]\n",
    "\n",
    "for i,predict in enumerate(pred):\n",
    "    y_pred.append(str(round(predict,4)))\n",
    "y_pred=np.array(y_pred)\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "output.to_csv('outputs/sub_auto_tuned{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "print (\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train another model with tuned parameters and smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params = dict()\n",
    "\n",
    "best_params['max_depth'] = int(BO_scores['max_depth'][0])\n",
    "best_params['min_child_weight'] = int(BO_scores['min_child_weight'][0])\n",
    "best_params['colsample_bytree'] = BO_scores['colsample_bytree'][0]\n",
    "best_params['subsample'] = BO_scores['subsample'][0]\n",
    "best_params['gamma'] = BO_scores['gamma'][0]\n",
    "\n",
    "best_params['objective'] = 'reg:linear'\n",
    "best_params['eta'] = 0.01\n",
    "best_params['seed'] = 1234\n",
    "\n",
    "\n",
    "cv_result = xgb.cv(best_params, xgtrain,\n",
    "                   num_boost_round=100000,\n",
    "                   nfold=5,\n",
    "                   metrics={'mae'},\n",
    "                   seed=1234,\n",
    "                   callbacks=[xgb.callback.early_stop(200)], # we need to increase stopping rounds too\n",
    "                  verbose_eval=50)\n",
    "\n",
    "best_iteration = len(cv_result)\n",
    "best_score = cv_result['test-mae-mean'].min()\n",
    "print(\"Best score {}, best iteration {}\".format(best_score,best_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.train(best_params, xgtrain, num_boost_round=best_iteration)\n",
    "pred = model.predict(xgtest)\n",
    "y_pred=[]\n",
    "\n",
    "for i,predict in enumerate(pred):\n",
    "    y_pred.append(str(round(predict,4)))\n",
    "y_pred=np.array(y_pred)\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "output.to_csv('outputs/sub_auto_tuned_small_eta{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "print (\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
